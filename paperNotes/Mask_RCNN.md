### å‰è¨€
çœ‹Mask-RCNNçš„æ—¶å€™é‡åˆ°äº†å¾ˆå¤šä»¥å‰è®ºæ–‡ä¸­çš„æ¦‚å¿µï¼Œç‰¹æ­¤åœ¨è¿™æ¬¡åšä¸ªæ•´åˆ,æ–¹ä¾¿æ›´å¥½çš„ç†è§£æ–‡ç« çš„æ¦‚å¿µã€‚

### Faster-RCNN
[Faster-RCNNè®ºæ–‡åœ°å€](https://arxiv.org/abs/1506.01497)  
é¦–å…ˆæ¥æFaster-RCNNç½‘ç»œç»“æž„ï¼Œæ˜¯å› ä¸ºMask-RCNNæ˜¯åœ¨å…¶åŸºç¡€ä¸Šæ”¹è¿›ç½‘ç»œç»“æžœã€Œæ›´å…·ä½“ç‚¹å¹¶åˆ—åŠ ä¸€ä¸ªmask branchã€è€Œå¾—åˆ°çš„æ¥å®žçŽ°segmentationã€‚
Faster-RCNNåœ¨object detectionä¸­ç›¸å½“äºŽ**baseline system**ï¼Œä¹Ÿæ˜¯**benchmark**ã€‚ä¸»è¦åŒ…æ‹¬äº†å¯¹ç›®æ ‡ç‰©ä½“çš„åˆ†ç±»ï¼ˆclassificationï¼‰ï¼Œä»¥åŠç”¨å€™é€‰æ¡†ï¼ˆbounding boxï¼‰æ¥å¯¹å›¾ç‰‡ä¸­çš„ä½ç½®è¿›è¡Œå®šä½ã€‚åœ¨æ­¤ä¹‹å‰ä¹Ÿå·²æœ‰äº†Fast-RCNNä¹‹ç±»çš„ç›®æ ‡æ£€æµ‹ç®—æ³•äº†ã€‚æ–‡ç« ã€ŒFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networksã€çš„åˆ›æ–°åœ¨äºŽè§£å†³äº†Region Proposalç”Ÿæˆå¼€é”€é—®é¢˜ã€‚å½“ç”Ÿæˆçš„å€™é€‰æ¡†è¿‡å¤šæ—¶ï¼Œprocessing speedä¼šå—åˆ°å½±å“ï¼Œä»Žè€Œæ²¡æ³•å¾ˆå¥½çš„å®žçŽ°**real-time object detection**ã€‚
> we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly **cost-free** region proposals

åœ¨Faster-RCNNä¸­ä½¿ç”¨RPNæ¥è¿›è¡Œå€™é€‰æ¡†çš„ç¡®å®šï¼Œå³ã€ŒRegion proposal Networkæ‰¾å‡ºç‰©ä½“å¯èƒ½å­˜åœ¨çš„æ‰€æœ‰ä½ç½®ã€ï¼Œåœ¨è¿™ä¸€ä¸ªè¿‡ç¨‹ä¸­æ‰¾å…¨ï¼Œæ²¡æœ‰æ¼æ£€å¾ˆé‡è¦ï¼Œä¸ç„¶åŽé¢çš„åˆ†ç±»ä¹Ÿæ²¡æ³•åˆ†äº†ã€‚å³Recallçš„å€¼è¦é«˜ï¼Œã€ŒRecall=æ­£ç¡®è¯†åˆ«å‡ºæ¥çš„object/æ•°æ®åº“é‡Œå«æœ‰çš„objectï¼Œå½“recall=100%æ—¶ï¼Œè¡¨ç¤ºæ²¡æœ‰æ¼æ£€ã€ã€‚RPNç½‘ç»œæ˜¯ä¸€ç§å…¨è¿žæŽ¥ç½‘ç»œï¼ˆFCNåœ¨ä¸‹æ–‡æœ‰æåˆ°å“ˆå“ˆï¼‰

RPNé¢„æµ‹äº†object bounds and objectness scores at each positionï¼Œè¿™ç»™Fast-RCNNèµ·åˆ°**ç±»ä¼¼æŒ‡å“ªæ‰“å“ª**çš„ä½œç”¨äº†ã€‚æ­¤å¤–ï¼Œè¿™é‡Œä¹Ÿæ˜¯æ–‡ç« çš„å¦ä¸€ä¸ª**åˆ›æ–°ç‚¹**ï¼Œé€šè¿‡ã€Œsharing the convolutional featuresã€å®žçŽ°äº†RPNå’ŒFast-RCNNèžåˆåˆ°ä¸€ä¸ªç½‘ç»œä¸­åŽ»äº†ã€‚  
åœ¨è¿™é‡Œæˆ‘ä»¬æ¥ç†è§£ä¸€ä¸‹ã€Œsharingã€ï¼ŒRPNä»Žfeature map ä¸Šé€‰æ‹©å‡ºäº†ä¸€ç³»åˆ—çš„bounding boxï¼Œç„¶åŽFast-RCNNå†æ¬¡åˆ©ç”¨äº†feature mapï¼Œå¹¶ç”¨ROI poolingï¼ˆ*ä¸»è¦åŒ…æ‹¬ä¸‰æ­¥ï¼š1. æŠŠ region proposal åˆ†ä¸ºnç­‰åˆ†ï¼Œn=the dimension of the output  2. æ‰¾åˆ°æ¯ä¸ªsectionæœ€å¤§çš„å€¼  3.æŠŠæ¯ä¸ªæœ€å¤§çš„æå–å‡ºæ¥ä½œä¸ºoutput buffer*ï¼‰æ¥å¯¹æ¯ä¸ªcandidate boxè¿›è¡Œclassification å’Œ bounding box regressionï¼Œä¹Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸ŠèŠ‚çœäº†è®¡ç®—å¼€é”€ï¼ŒåŠ é€Ÿäº†è®­ç»ƒè¿‡ç¨‹ã€‚
> using the recently popular terminology of neural networks with â€œattentionâ€ mechanisms, the RPN component tells the unified network where to look  
![Faster-RCNN](https://lilianweng.github.io/lil-log/assets/images/faster-RCNN.png)  



### Feature Pyramid Network
[FPNè®ºæ–‡åœ°å€](https://arxiv.org/abs/1612.03144)

é—®é¢˜ï¼š  
ROIæ˜ å°„åˆ°æŸä¸ªfeature mapæ˜¯å°†åº•å±‚çš„åæ ‡ç›´æŽ¥é™¤ä»¥strideï¼Œæ˜¾ç„¶å¯¹äºŽå°ç›®æ ‡ï¼ˆsizeæ¯”è¾ƒå°ï¼‰ç‰©ä½“æ¥è¯´ï¼Œåˆ°åŽé¢çš„å·ç§¯æ± åŒ–æ—¶ï¼Œå®žé™…çš„è¯­ä¹‰ä¿¡æ¯å°±ä¸¢å¤±äº†å¾ˆå¤šäº†ã€‚FPNè§£å†³çš„æ˜¯å¤šå°ºåº¦æ£€æµ‹çš„é—®é¢˜ã€‚ 

ç»“æž„ï¼š  
FPNåˆ©ç”¨äº†CNNå±‚çº§ç‰¹å¾çš„é‡‘å­—å¡”å½¢å¼ï¼ŒåŒæ—¶ç”Ÿæˆåœ¨æ‰€æœ‰å°ºåº¦ä¸Šå…·æœ‰å¼ºè¯­ä¹‰ä¿¡æ¯çš„ç‰¹å¾é‡‘å­—å¡”ã€‚FPNè®¾è®¡çš„é‡‘å­—å¡”ç»“æž„åŒ…æ‹¬äº†bottom-up & top-down & lateral connections(æ¨ªå‘è¿žæŽ¥)ä¸‰ç§ç»“æž„ã€‚  
1. bottom-upæ˜¯ä¸»å¹²CNNæ²¿å‰å‘ä¼ è¾“ï¼ˆfeed-foward/inference)çš„æ—¶å€™äº§ç”Ÿçš„ä¸€ç³»åˆ—ä¸åŒå°ºåº¦çš„feature mapã€‚é€šå¸¸æ˜¯æ¯ä¸ªé˜¶æ®µæœ€æ·±çš„å±‚æœ‰strongest featuresã€‚  
2. top-downæ˜¯å‘ä¸Šé‡‡æ ·ï¼ˆupsampling)  
3. lateral connectionå¸®åŠ©èžåˆä¸åŒå±‚çš„è¯­ä¹‰ä¿¡æ¯ï¼ˆå³èžåˆäº†bottom-upå’Œtop-downçš„è¯­ä¹‰ä¿¡æ¯ï¼‰ï¼Œè¾¾åˆ°å•å°ºåº¦å•å¼ inputï¼Œæž„å»ºmultiple scaleçš„ç‰¹å¾é‡‘å­—å¡”ã€‚
 æ­¤å¤–ï¼Œä½¿ç”¨äº†1x1çš„å·ç§¯æ ¸æ¥èµ·åˆ°é™ä½Žç»´åº¦çš„ä½œç”¨ã€‚
 ![FPN](https://www.pytorchtutorial.com/wp-content/uploads/2018/08/1174793-20170612173455400-159085110.png)
**FPNåœ¨mask-RCNNä¸­çš„ç”¨æ³•**
ä»ŽðŸ‘†æˆ‘ä»¬ä¹Ÿå·²ç»çœ‹åˆ°ä»Žå•ä¸€å°ºåº¦çš„å›¾åƒè¾“å…¥ä¸­ï¼ŒFPNå¯ä»¥èŽ·å–multi-scaleçš„ç‰¹å¾å›¾ã€‚åœ¨Mask-RCNNä¸­ä½œè€…é‡‡ç”¨ResNet-FPNä½œä¸ºä¸»å¹²çš„ç½‘ç»œç»“æž„ã€‚åŽŸæ–‡æ˜¯è¿™ä¹ˆæè¿°çš„ã€‚
> Using a ResNet-FPN backbone for feature extraction with Mask R-CNN gives excellent gains in both accuracy and speed
### Fully Convolutional Networks
[FCNè®ºæ–‡åœ°å€](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf) 

ã€ŒFully Convolutional Networks for semantic segmentationã€ä¸€å¼€å§‹è¯´çš„ï¼š
> combines semantic information from a ã€Œdeep , coarseã€ layer with appearance information from a ã€Œshallow, fineã€ layers  

é‚£ä¸ºä»€ä¹ˆdeepå’Œcoarseè¿žåœ¨ä¸€èµ·ï¼Œshallowå’Œfineæ¥æŽ¥åœ¨ä¸€èµ·ï¼Œä¸æ˜¯è¶Šdeepçš„å±‚ï¼Œè¶Šæœ‰è¡¨è¾¾åŠ›ä¹ˆï¼Ÿ  

**å…¨è¿žæŽ¥ç½‘ç»œå’ŒCNNä¹‹é—´çš„åŒºåˆ«**ï¼š  
ç»å…¸çš„CNNæ˜¯å°†å·ç§¯å±‚äº§ç”Ÿçš„feature mapä½¿ç”¨å…¨è¿žæŽ¥å±‚æ˜ å°„ä¸ºå›ºå®šé•¿åº¦çš„ç‰¹å¾å‘é‡ï¼Œæœ€åŽè¾“å‡ºçš„æ˜¯æ¦‚çŽ‡ã€‚
FCNå°†å…¨è¿žæŽ¥å±‚éƒ½å˜åŒ–ä¸ºå·ç§¯å±‚ï¼Œã€ŒE.X.: å°†4096 å˜æˆ1x1x4096ã€æ˜¯é’ˆå¯¹è¯­ä¹‰åˆ†å‰²è®­ç»ƒçš„ä¸€ä¸ªend-to-end, pixelçš„ç½‘ç»œï¼Œæœ€åŽè¾“å‡ºçš„æ˜¯heatmapçƒ­åŠ›å›¾ã€‚  

**FCNç½‘ç»œç»“æž„åˆ›æ–°ç‚¹**ï¼š
FCNå¯ä»¥æŽ¥å—ä»»æ„å°ºå¯¸çš„è¾“å…¥å›¾åƒï¼Œé‡‡ç”¨åå·ç§¯å±‚å¯¹æœ€åŽä¸€ä¸ªå·ç§¯å±‚çš„feature mapåšä¸Šé‡‡æ ·upsamplingï¼Œ
ä½¿å…¶æ¢å¤åˆ°è¾“å…¥å›¾åƒçš„ç›¸åŒå°ºå¯¸ï¼Œä»Žè€Œå¯¹æ¯ä¸€ä¸ªåƒç´ éƒ½äº§ç”Ÿä¸€ä¸ªé¢„æµ‹ï¼ŒåŒæ—¶ä¿ç•™åŽŸå§‹è¾“å…¥å›¾åƒçš„ç©ºé—´
ä¿¡æ¯ã€‚ä½†æ˜¯è¿™æ ·å¾—åˆ°çš„ç»“æžœæ¯”è¾ƒcoarser, ä¸€äº›ç»†èŠ‚ä¸èƒ½æ¢å¤ã€‚å› æ­¤ï¼Œä½œè€…é‡‡ç”¨äº†skip architectureæ¥ä¼˜åŒ–ä¸Šé‡‡æ ·ï¼Œå³å°†ä¸åŒæ± åŒ–å±‚çš„ç»“æžœè¿›è¡Œä¸Šé‡‡æ ·ï¼Œç„¶åŽç»“åˆè¿™äº›ç»“æžœæ¥ä¼˜åŒ–è¾“å‡ºã€‚
ã€ŒE.X ç¬¬äº”å±‚çš„è¾“å‡º32å€æ”¾å¤§åå·ç§¯åˆ°åŽŸå›¾å¤§å°æ—¶æ¯”è¾ƒç²—ç³™ï¼Œå› æ­¤ä½œè€…å°†ç¬¬å››å±‚è¾“å‡º16å€æ”¾å¤§ï¼Œç¬¬3å±‚è¾“å‡º8å€æ”¾å¤§ï¼Œå¯ä»¥ä»ŽåŽŸè®ºæ–‡ä¸­æ’å›¾çœ‹åˆ°è¶Šä½Žæ± åŒ–å±‚ï¼Œè¶Šç²¾ç»†ã€
å› æ­¤æˆ‘ä»¬ä¹Ÿå°±å¯ä»¥ç†è§£äº†ä¸Šæ–‡çš„é—®é¢˜ã€‚

**FCNåœ¨mask-RCNNä¸­çš„åº”ç”¨**ï¼š
åœ¨the mask branchä¸­ï¼ŒFCNè¢«ç”¨åœ¨æ¯ä¸ªROIä¸­è¿›è¡Œpixel-to-pixelçš„åˆ†å‰²ï¼Œè¿™ä¹Ÿæ˜¯mask-RCNNè¶…è¶Šäº†Faster-RCNNçš„åœ°æ–¹ã€‚
ä½œè€…åœ¨æ–‡ç« é‡Œæ˜¯è¿™ä¹ˆè¯´çš„ï¼š
> Our method, called Mask-RCNNï¼Œextends Faster-RCNN by adding a branch for predicting segmentation masks on each Region of Interest,in parallel with the existing branch for classification and bounding box regression.

### Mask-RCNN
[Mask-RCNNè®ºæ–‡åœ°å€](https://arxiv.org/abs/1703.06870)  

Mask-RCNNå®žçŽ°çš„ä»»åŠ¡è¦æ›´ã€Œéš¾ã€ï¼Œå› ä¸ºä¸å†æ˜¯object detection è€Œæ˜¯è¦è¾¾åˆ°instance segmentationï¼Œç»†åŒ–åˆ°åŒºåˆ†ç±»åˆ«ä¸­çš„ä¸åŒå®žä¾‹ã€‚é€šä¿—ç‚¹è¯´ï¼Œåƒç´ åˆ†ç±»çš„è¯å¯ä»¥ç”¨ä¸åŒçš„é¢œè‰²æ¥åŒºåˆ«ä¸åŒçš„å®žä¾‹ï¼Œä½†æ˜¯å®žä¾‹åˆ†å‰²çš„æ—¶å€™å³ä½¿æ˜¯åŒä¸€ç§ç±»çš„ç‰©ä½“ï¼Œæ¯”å¦‚éƒ½æ˜¯çŒ«çŒ«ï¼Œä¹Ÿè¦åŒºåˆ«å‡ºæ©˜çŒ«å’ŒåŠ è²çŒ«ã€‚åƒFCNä¸­ä¹Ÿå¯ä»¥ç”¨åœ¨å®žä¾‹åˆ†å‰²çš„æƒ…æ™¯ä¸­ï¼Œä½†å®ƒä»¬çš„åšæ³•æ˜¯ï¼Œå¯¹æ¯ä¸ªåƒç´ è¿›è¡Œmulti-class categorizationã€‚
ä½œè€…æå‡ºçš„æ–¹æ³•åœ¨å®žä¾‹åˆ†å‰²ä¸­æ˜¯æ›´æœ‰ä¼˜åŠ¿çš„ã€‚
> Instead, our method is based on parallel prediction of masks and class labels, which is **simpler and more flexible**.
> In contrast to the segmentation-first level of these methods, Mask R-CNN is based on an **instance first strategy**.  

åœ¨ä¸Šé¢ä»‹ç»faster-RCNNæ—¶ï¼Œå·²ç»æåˆ°äº†Mask-RCNNå¢žåŠ äº†åˆ†æ”¯ï¼Œæ¥é¢„æµ‹ç‰©ä½“å¯¹åº”çš„æŽ©è†œ(object mask).  

åœ¨é˜…è¯»Mask-RCNNçš„æ—¶å€™ï¼Œé‡åˆ°ä¸€ä¸ªé—®é¢˜ã€Œå¦‚ä½•æ¥ç†è§£**pixel-to-pixel alignment** ã€
> we propose a simple, quantiazation-free layer, called *ROIAlign*ï¼Œ that preserves exact spatial locations.   

ç›¸æ¯”faster-RCNNçš„ROI Pooling, Mask-RCNN ç”¨çš„æ–¹æ³•æ˜¯ROIAlign

