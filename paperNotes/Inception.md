Inceptionç³»åˆ—æœ‰å››ç¯‡é‡è¦çš„paperï¼Œåˆ†åˆ«æ˜¯ï¼š[Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)ã€
[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shif](https://arxiv.org/abs/1502.03167)ã€[Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567)ã€[Inception-v4](https://arxiv.org/abs/1602.07261)
åœ¨æ­¤ï¼Œä¾æ¬¡é˜…è¯»å¹¶åšç¬”è®°ã€‚
## GoogleNet
### Introduction & Motivation
[Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842) é¦–æ¬¡æå‡ºäº†ã€Œ**Inception**ã€æ¨¡å—ä½œä¸ºç½‘ç»œæ„æ¶ï¼Œ
è¯¥ç½‘ç»œæ„æ¶ä¹Ÿæ˜¯åç»­ä½œä¸ºclassificationå’Œdetectionçš„base networkçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚
> we introduce a new level of organization in the form of the "Inception module" and also in a more direct sense of increased
network depth.  

ç½‘ç»œçš„sizeä¸»è¦ä»ä¸¤æ–¹é¢è¿›è¡Œè€ƒè™‘ï¼šdepth - the number of levels - of the network å’Œ width - the number of units at each levelã€‚ã€ŒåŠ æ·±ç½‘ç»œdepthã€ã€ã€Œè°ƒèŠ‚è¶…å‚æ•°ã€å¯ä»¥åœ¨recognitionå’Œobject detectionå–å¾—æ›´å¥½çš„æ•ˆæœã€‚ä½†æ˜¯ç½‘ç»œçš„sizeè¿‡å¤§ï¼Œä¼šç›´æ¥å½±å“è¿è¡Œçš„æ€§èƒ½ã€‚å°±åƒä¸€ä¸ªäººè¿‡èƒ–ï¼Œä¼šç›´æ¥å½±å“èº«ä½“å¥åº·ã€‚å½“ç½‘ç»œçš„sizeè¿‡å¤§çš„æ—¶å€™ï¼Œå‚æ•°#paramtersè¿‡å¤šï¼Œæ¶ˆè€—çš„è®¡ç®—èµ„æºå°±è¶Šå¤šï¼Œæ­¤å¤–ï¼Œç‰¹åˆ«æ˜¯åœ¨labeled exampleså¾ˆæœ‰é™çš„æƒ…å†µä¸‹ï¼Œæ›´å®¹æ˜“å‡ºç°overfittingã€‚ä¸€èˆ¬æ˜¯é‡‡ç”¨ã€Œdropoutã€æˆ–è€…ã€Œregularizationã€ï¼Œå¹¶ä¸”ã€Œè°ƒæ•´è¶…å‚æ•°ã€å’Œã€Œè®¾ç½®å­¦ä¹ ç‡ã€å»é˜²æ­¢è®­ç»ƒè¿‡ç¨‹ä¸­è¿‡æ‹Ÿåˆç°è±¡çš„å‡ºç°ã€‚
> For larger datasets such as Imagenet, deeper architectures are used to get better results and dropout is used to prevent
overfitting â€¦â€¦ Since in practice the computational budget is always finite, an efficient distribution of computing resources is preferred to an indiscriminate increase of sizeã€‚  

### Inception module
ä¸Šé¢çš„æ–¹æ³•æŒºå¥½çš„ï¼Œä½†ä¹ŸæŒºè›®çƒ¦çš„ï¼Œæ‰€ä»¥ä½œè€…è¯•å›¾ç»“åˆã€Œæ•°æ®ç»“æ„ã€ç½‘ç»œç»“æ„ã€æ¥è€ƒè™‘ï¼Œå¦‚ä½•è®¾è®¡ä¸€ä¸ªåˆ›æ–°æ€§çš„architectureï¼Œæ¥æ›´å¥½çš„åˆ©ç”¨è®¡ç®—èµ„æºä»¥åŠç¨å¾®æ”¾å¿ƒã€å¤§èƒ†çš„è®¾ç½®å‚æ•°ä¸€äº›?

é¦–å…ˆï¼Œå¼€ä¸ªå°åˆ†æ”¯ï¼Œä»‹ç»ä¸€ä¸‹ã€Œç¨€ç–ç»“æ„ã€çš„ç†è®ºåŸºç¡€ï¼š**HebbianåŸç†**ã€‚ ä½œè€…ä»neuroscienceçš„è§’åº¦å¾—åˆ°äº†å¯å‘ï¼Œæå‡ºäº†ç½‘ç»œç»“æ„çš„åˆ›æ–°ï¼šã€‚
è¯¥åŸç†æŒ‡å‡ºï¼šå„ä¸ªç¥ç»å…ƒæ˜¯ç»„åˆæ•ˆåº”ï¼Œé€šè¿‡ç¥ç»çªè§¦è¿›è¡Œä¿¡æ¯çš„ä¼ é€’ï¼Œå¤§è„‘çš®å±‚æ¥æ”¶ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œ
ç¥ç»åå°„æ´»åŠ¨çš„æŒç»­ä¸é‡å¤ä¼šå¯¼è‡´ç¥ç»å…ƒè¿æ¥ç¨³å®šæ€§çš„æŒä¹…æå‡ï¼Œå½“ä¸¤ä¸ªç¥ç»å…ƒç»†èƒAå’ŒBè·ç¦»å¾ˆè¿‘ï¼Œå¹¶ä¸”Aå‚ä¸äº†å¯¹Bé‡å¤ã€æŒç»­çš„å…´å¥‹ï¼Œé‚£ä¹ˆæŸäº›ä»£è°¢å˜åŒ–ä¼šå¯¼è‡´Aå°†ä½œä¸ºèƒ½ä½¿Bå…´å¥‹çš„ç»†èƒã€‚
> neurons that fire together, wire together.
å°†Fully Connectedå˜ä¸ºç¨€ç–è¿æ¥ï¼ˆsparse connectionï¼‰çš„æ—¶å€™ï¼Œå¯ä»¥åœ¨å¢åŠ ç½‘ç»œæ·±åº¦å’Œå®½åº¦çš„åŒæ—¶å‡å°‘å‚æ•°ä¸ªæ•°ï¼Œ
ä½†æ˜¯å¤§éƒ¨åˆ†çš„ç¡¬ä»¶æ˜¯é’ˆå¯¹å¯†é›†çŸ©é˜µè®¡ç®—ä¼˜åŒ–çš„ï¼Œç¨€ç–çŸ©é˜µè™½ç„¶æ•°æ®é‡å˜å°‘ï¼Œä½†è®¡ç®—æ‰€æ¶ˆè€—çš„æ—¶é—´å¾ˆéš¾å‡å°‘ã€‚
GoogleNetå¸Œæœ›åšçš„å°±æ˜¯æ—¢ä¿è¯ç½‘ç»œç»“æ„çš„ç¨€ç–æ€§ã€åˆåˆ©ç”¨å¯†é›†çŸ©é˜µçš„é«˜è®¡ç®—æ€§èƒ½ã€‚

GoogleNetçš„æ ¸å¿ƒæ˜¯Inception moduleï¼Œè€ŒInceptionç›¸å½“äºä¸€ä¸ªConvolutional building blockï¼Œä¹Ÿæ˜¯ä¸€ä¸ªå±€éƒ¨ç¨€ç–æœ€ä¼˜è§£çš„ç½‘ç»œæ„æ¶ï¼Œç„¶åæˆ‘ä»¬åœ¨
ç©ºé—´ä¸Šåšå †å ã€‚ä¸‹é¢æˆ‘ä»¬ç»“åˆè®ºæ–‡çš„æ’å›¾æ¥ä»”ç»†åˆ†æä¸€ä¸‹Inception moduleã€‚å›¾aæ˜¯åŸå§‹çš„Inception moduleï¼Œå›¾bæ˜¯å€Ÿé‰´äº†NINï¼ˆNetwork In Networkï¼‰
å¼•å…¥1x1çš„å·ç§¯æ“ä½œï¼Œæ”¹è¿›åçš„Inception moduleã€‚
> Our network will be built from convolutional building blocks.
All we need is to find the **optimal local construction** and to repeat it spatially.   

![1](https://cdn-images-1.medium.com/max/2000/1*aq4tcBl9t5Z36kTDeZSOHA.png)  

è¾“å…¥æœ‰å››ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šä¸ªå°ºåº¦ï¼ˆ1x1æˆ–3x3æˆ–5x5ï¼‰çš„å·ç§¯å’Œæ± åŒ–è¿›è¡Œç‰¹å¾æå–ã€Œç›¸å½“äºå°†ç¨€ç–çŸ©é˜µåˆ†è§£ä¸ºå¯†é›†çŸ©é˜µã€ï¼Œæ¯ä¸€å°ºåº¦æå–çš„ç‰¹å¾æ˜¯å‡åŒ€åˆ†å¸ƒçš„ï¼Œ
ä½†æ˜¯ç»è¿‡ã€Œfilter concatenationã€è¿™æ­¥æ“ä½œåï¼Œè¾“å‡ºçš„ç‰¹å¾ä¸å†æ˜¯å‡åŒ€åˆ†å¸ƒçš„ï¼Œ**ç›¸å…³æ€§å¼ºçš„ç‰¹å¾ä¼šè¢«åŠ å¼ºï¼Œè€Œç›¸å…³æ€§å¼±çš„ç‰¹å¾ä¼šè¢«å¼±åŒ–**ã€‚*è¿™ä¸ªç›¸å…³æ€§é«˜çš„èŠ‚ç‚¹åº”è¯¥è¢«è¿æ¥åœ¨ä¸€èµ·çš„ç»“è®ºï¼Œå³æ˜¯ä»ç¥ç»ç½‘ç»œçš„è§’åº¦å¯¹HebbianåŸç†æœ‰æ•ˆæ€§çš„è¯æ˜*
ã€Œfilter concatenationã€ï¼Œè¿™ä¸€æ­¥å…¶å®ç›¸å½“äºæ²¿ç€æ·±åº¦æ–¹å‘ï¼ˆæˆ–è€…è¯´åœ¨depthè¿™ä¸ªç»´åº¦ï¼‰è¿›è¡Œæ‹¼æ¥ï¼Œè¾“å‡º a single output vector forming the input of 
next stageã€‚ç»“åˆ[Udacityè§†é¢‘](https://becominghuman.ai/understanding-and-coding-inception-module-in-keras-eb56e9056b4b)å’Œcodeæ¥åŠ æ·±ä¸€ä¸‹å¯¹ã€Œfilter concatenationã€çš„ç†è§£
> 
    concatenated_tensor = tf.concat(3,[branch1, branch2, branch3, branch 4])  

ç°åœ¨æˆ‘ä»¬æ¥åˆ†æä¸€ä¸‹ï¼Œä¸Šé¢çš„å›¾bç›¸æ¯”å›¾açš„ä¼˜åŠ¿åœ¨å“ªé‡ŒğŸ§ã€‚  
1x1å·ç§¯çš„ã€Œæ€§ä»·æ¯”ã€å¾ˆé«˜ï¼Œç”¨å¾ˆå°çš„è®¡ç®—é‡å¯ä»¥å¢åŠ ä¸€å±‚ç‰¹å¾å˜æ¢å’Œéçº¿æ€§å˜æ¢ã€‚
å®ƒçš„è®¡ç®—é‡å°è¡¨ç°åœ¨ï¼Ÿ
### GoogleNet's architecture
é¦–å…ˆï¼Œä¸ºäº†æœ‰ä¸€ä¸ªåˆæ­¥çš„å°è±¡ï¼Œå…ˆæˆªå–äº†GoogleNetçš„ä¸€éƒ¨åˆ†ï¼Œ
![2](https://mohitjainweb.files.wordpress.com/2018/06/googlenet-architecture-showing-the-side-connection.png?w=700)  
æˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ°è¿™é‡Œæœ‰ä¸€ä¸ªã€Œsoftmaxã€çš„åˆ†æ”¯ï¼Œæ•´ä¸ªç»“æ„ä¸­æœ‰ä¸¤ä¸ªã€Œsoftmaxã€ï¼Œå®ƒç›¸å½“äºè¾…åŠ©åˆ†ç±»å™¨ï¼Œç»“åˆcodeæˆ‘ä»¬å¯ä»¥çŸ¥é“è¯¥æ“ä½œæ˜¯å°†ä¸­é—´æŸä¸€å±‚çš„è¾“å‡ºç”¨ä½œåˆ†ç±»ï¼Œå¹¶æŒ‰ä¸€ä¸ªè¾ƒå°çš„æƒé‡ï¼ˆ0.3ï¼‰åŠ åˆ°æœ€ç»ˆåˆ†ç±»ç»“æœä¸­èµ·åˆ°çš„æ˜¯æ¢¯åº¦å‰å‘ä¼ è¾“çš„ä½œç”¨ã€‚è®ºæ–‡é‡Œæ˜¯è¿™ä¹ˆäº¤ä»£çš„ï¼š
> By adding auxiliary classifiers connected to these intermediate layers, we would expect to encourage discrimination in the lower stages in the classifier, increase the gradient signal that gets propagated back, and provide additional regularization.  

å…¶æ¬¡ï¼Œä¸ºäº†å¯¹GoogleNetçš„ç»„æˆæœ‰ä¸€ä¸ªæ¦‚å¿µï¼Œå¼•ç”¨äº†è®ºæ–‡ä¸­çš„è¡¨æ ¼ï¼š
![3](https://img-blog.csdn.net/20170612110458444?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbWFyc2poYW8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
ä¸Šé¢è®¨è®ºæ—¶ï¼Œå·²ç»è¯´è¿‡GoogleNetæ˜¯æ¨¡å—åŒ–çš„ï¼Œå †å äº†å¤šä¸ªInception Moduleï¼Œé åçš„Inception Moduleèƒ½å¤ŸæŠ½å–æ›´é«˜é˜¶çš„æŠ½è±¡çš„ç‰¹å¾ã€‚

